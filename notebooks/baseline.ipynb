{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_prepaired = '../dataset/dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(path_data_prepaired) as file_data:\n",
    "    data = json.load(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_sentences(data):\n",
    "    \"\"\"\n",
    "        Cleaning sentences, removing special characters and articles\n",
    "    \"\"\"\n",
    "    sentences = list()\n",
    "    for record in data:\n",
    "        sentence = record['reviewText']\n",
    "        sentence = sentence.lower()\n",
    "        for char in \"?.!/;:,\":\n",
    "            sentence = sentence.replace(char, '')\n",
    "\n",
    "        sentence = sentence.split(sep=' ')\n",
    "        sentence = [word for word in sentence if len(word) > 1]\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.67 s, sys: 2.41 s, total: 9.08 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%time sentences = clear_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences, r=200):\n",
    "    vocabulary = dict()\n",
    "    word_count = dict()\n",
    "    num = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "    \n",
    "    for word, count in word_count.items():\n",
    "        if word_count[word] >= r:\n",
    "            vocabulary[word] = num\n",
    "            num += 1\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_matrix(sentences, vocabulary, L = 2):\n",
    "    words_counts = Counter()\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        for word_index, word in enumerate(sentence):\n",
    "            if word in vocabulary:\n",
    "                around_indexes = [i for i in range(max(word_index - L, 0), \n",
    "                                                   min(word_index + L + 1, len(sentence))) \n",
    "                                  if i != word_index]\n",
    "                for occur_word_index in around_indexes:\n",
    "                        occur_word = sentence[occur_word_index]\n",
    "                        if occur_word in vocabulary:\n",
    "                            skipgram = (word, occur_word)\n",
    "                            if skipgram in words_counts:\n",
    "                                words_counts[skipgram] += 1\n",
    "                            else:\n",
    "                                words_counts[skipgram] = 1\n",
    "    rows = list()\n",
    "    cols = list()\n",
    "    values = list()\n",
    "    \n",
    "    \n",
    "    for (word_1, word_2), sharp in words_counts.items():                                            \n",
    "        rows.append(vocabulary[word_1])\n",
    "        cols.append(vocabulary[word_2])\n",
    "        values.append(sharp)\n",
    "    \n",
    "    corpus_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "    \n",
    "    return corpus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.08 s, sys: 2.04 ms, total: 4.08 s\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%time vocabulary = create_vocabulary(sentences, r=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of prototype embedding_computer function\n",
    "def embedding_computer(corpus_matrix, vocabulary, **kwargs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(corpus_matrix, k, d=200):\n",
    "    \"\"\"\n",
    "        PS2 embedding_computer from HW, one more example\n",
    "    \"\"\"\n",
    "    all_observations = corpus_matrix.sum()\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    sppmi_values = []\n",
    "\n",
    "    sum_over_words = np.array(corpus_matrix.sum(axis=0)).flatten()\n",
    "    sum_over_contexts = np.array(corpus_matrix.sum(axis=1)).flatten()\n",
    "\n",
    "    for word_index_1, word_index_2 in zip(corpus_matrix.nonzero()[0], \n",
    "                                          corpus_matrix.nonzero()[1]):\n",
    "\n",
    "        sg_count = corpus_matrix[word_index_1, word_index_2]\n",
    "\n",
    "        pwc = sg_count\n",
    "        pw = sum_over_contexts[word_index_1]\n",
    "        pc = sum_over_words[word_index_2]\n",
    "\n",
    "        spmi_value = np.log2(pwc * all_observations / (pw * pc * k))\n",
    "        sppmi_value = max(spmi_value, 0)\n",
    "\n",
    "        rows.append(word_index_1)\n",
    "        cols.append(word_index_2)\n",
    "        sppmi_values.append(sppmi_value)\n",
    "\n",
    "    sppmi_mat = sparse.csr_matrix((sppmi_values, (rows, cols)))\n",
    "    U, S, V = linalg.svds(sppmi_mat, 200)\n",
    "    embedding_matrix = U @ scipy.diag(scipy.sqrt(S))\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecController:\n",
    "    \n",
    "    def compute_embeddings(corpus_matrix=None, vocabulary=None, embedding_computer=None):\n",
    "        \"\"\"\n",
    "            @param corpus_matrix - [w_i, c_i], as it was in second PS, \n",
    "                                    generated by create_corpus_matrix (maybe it will be \n",
    "                                    generated by another function with similar prototype)\n",
    "            @param vocabulary - it is obvious, what is it\n",
    "            @param embedding_computer - function, which return embedding matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # preparation and execution embeggin_computer function\n",
    "        # embedding matrix\n",
    "        \n",
    "        # self.embedding = ...\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calc_cosine_similarity(word):\n",
    "        \"\"\"\n",
    "            Cosine similarity calculator (maybe from sklearn)\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculation similarity\n",
    "        # similarity = cosine(word, self.embedding)\n",
    "        \n",
    "        return similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
