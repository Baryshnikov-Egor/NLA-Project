{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_prepaired = '../dataset/dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(path_data_prepaired) as file_data:\n",
    "    data = json.load(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_sentences(data):\n",
    "    \"\"\"\n",
    "        Cleaning sentences, removing special characters and articles\n",
    "    \"\"\"\n",
    "    sentences = list()\n",
    "    for record in data:\n",
    "        sentence = record['reviewText']\n",
    "        sentence = sentence.lower()\n",
    "        for char in \"?.!/;:,\":\n",
    "            sentence = sentence.replace(char, '')\n",
    "\n",
    "        sentence = sentence.split(sep=' ')\n",
    "        sentence = [word for word in sentence if len(word) > 1]\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.63 s, sys: 563 ms, total: 8.2 s\n",
      "Wall time: 8.2 s\n"
     ]
    }
   ],
   "source": [
    "%time sentences = clear_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for creating bag of words models and corpus matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences, r=200):\n",
    "    vocabulary = dict()\n",
    "    word_count = dict()\n",
    "    num = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "    \n",
    "    for word, count in word_count.items():\n",
    "        if word_count[word] >= r:\n",
    "            vocabulary[word] = num\n",
    "            num += 1\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_matrix(sentences, vocabulary, L = 2):\n",
    "    words_counts = Counter()\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        for word_index, word in enumerate(sentence):\n",
    "            if word in vocabulary:\n",
    "                around_indexes = [i for i in range(max(word_index - L, 0), \n",
    "                                                   min(word_index + L + 1, len(sentence))) \n",
    "                                  if i != word_index]\n",
    "                for occur_word_index in around_indexes:\n",
    "                        occur_word = sentence[occur_word_index]\n",
    "                        if occur_word in vocabulary:\n",
    "                            skipgram = (word, occur_word)\n",
    "                            if skipgram in words_counts:\n",
    "                                words_counts[skipgram] += 1\n",
    "                            else:\n",
    "                                words_counts[skipgram] = 1\n",
    "    rows = list()\n",
    "    cols = list()\n",
    "    values = list()\n",
    "    \n",
    "    \n",
    "    for (word_1, word_2), sharp in words_counts.items():                                            \n",
    "        rows.append(vocabulary[word_1])\n",
    "        cols.append(vocabulary[word_2])\n",
    "        values.append(sharp)\n",
    "    \n",
    "    corpus_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "    \n",
    "    return corpus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.15 s, sys: 6.97 ms, total: 5.16 s\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "%time vocabulary = create_vocabulary(sentences, r=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding calculators, custom realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of prototype embedding_computer function\n",
    "def embedding_computer(corpus_matrix, vocabulary, **kwargs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(corpus_matrix, k, d=200):\n",
    "    \"\"\"\n",
    "        PS2 embedding_computer from HW, one more example\n",
    "    \"\"\"\n",
    "    all_observations = corpus_matrix.sum()\n",
    "\n",
    "    rows = []\n",
    "    cols = []\n",
    "    sppmi_values = []\n",
    "\n",
    "    sum_over_words = np.array(corpus_matrix.sum(axis=0)).flatten()\n",
    "    sum_over_contexts = np.array(corpus_matrix.sum(axis=1)).flatten()\n",
    "\n",
    "    for word_index_1, word_index_2 in zip(corpus_matrix.nonzero()[0], \n",
    "                                          corpus_matrix.nonzero()[1]):\n",
    "\n",
    "        sg_count = corpus_matrix[word_index_1, word_index_2]\n",
    "\n",
    "        pwc = sg_count\n",
    "        pw = sum_over_contexts[word_index_1]\n",
    "        pc = sum_over_words[word_index_2]\n",
    "\n",
    "        spmi_value = np.log2(pwc * all_observations / (pw * pc * k))\n",
    "        sppmi_value = max(spmi_value, 0)\n",
    "\n",
    "        rows.append(word_index_1)\n",
    "        cols.append(word_index_2)\n",
    "        sppmi_values.append(sppmi_value)\n",
    "\n",
    "    sppmi_mat = sparse.csr_matrix((sppmi_values, (rows, cols)))\n",
    "    U, S, V = linalg.svds(sppmi_mat, 200)\n",
    "    embedding_matrix = U @ scipy.diag(scipy.sqrt(S))\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for comparison and unified executing all of embedding computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecController:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_embeddings(corpus_matrix=None, vocabulary=None, embedding_computer=None):\n",
    "        \"\"\"\n",
    "            @param corpus_matrix - [w_i, c_i], as it was in second PS, \n",
    "                                    generated by create_corpus_matrix (maybe it will be \n",
    "                                    generated by another function with similar prototype)\n",
    "            @param vocabulary - it is obvious, what is it\n",
    "            @param embedding_computer - function, which return embedding matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # preparation and execution embeggin_computer function\n",
    "        # embedding matrix\n",
    "        \n",
    "        # self.embedding = ...\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calc_cosine_similarity(word):\n",
    "        \"\"\"\n",
    "            Cosine similarity calculator (maybe from sklearn)\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculation similarity\n",
    "        # similarity = cosine(word, self.embedding)\n",
    "        \n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for converting Sentences with Word2Vec class into vectors for representing sentences in vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecController:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def create_document_embedding_matrix(word2vec_controller, data, **kwargs):\n",
    "        \"\"\"\n",
    "            Create matrix with obervations for sklearn._predictor\n",
    "            @param word2vec_controller -\n",
    "            @param data - clear data with feedback and grades for constructing doc2vec matrix\n",
    "            @kwargs - any other params for model\n",
    "        \"\"\"\n",
    "        \n",
    "        # obsevations - matrix 2d np.ndarray (n, m + 1,) n - words count, m - contexts count, and grade of feedback\n",
    "        \n",
    "        return obervations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison models of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 base models, which included into our experiments for classifient feedbacks grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
