{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clear_sentences, create_vocabulary, create_corpus_matrix, compute_embeddings\n",
    "from utils import Word2VecController, Doc2VecController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_prepaired = '../dataset/dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(path_data_prepaired) as file_data:\n",
    "    data = json.load(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 237 ms, total: 3.25 s\n",
      "Wall time: 3.25 s\n"
     ]
    }
   ],
   "source": [
    "%time sentences = clear_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for creating bag of words models and corpus matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.48 s, sys: 2.49 ms, total: 2.49 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%time vocabulary = create_vocabulary(sentences, r=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.5 s, sys: 108 ms, total: 52.6 s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%time corpus_matrix = create_corpus_matrix(sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding calculators, custom realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 13.2 s, total: 1min 58s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%time embedding_matrix = compute_embeddings(corpus_matrix=corpus_matrix, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for comparison and unified executing all of embedding computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Riemannian Optimization\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary, corpus_matrix - we have before we start optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_grad_matrix(X, corpus_matrix, k=3):\n",
    "    \n",
    "    def sigmoid_fun(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    grad_F = np.zeros(corpus_matrix.shape)\n",
    "    \n",
    "    all_observations = corpus_matrix.sum()\n",
    "    sum_over_words = np.array(corpus_matrix.sum(axis=0)).flatten()\n",
    "    sum_over_contexts = np.array(corpus_matrix.sum(axis=1)).flatten()\n",
    "\n",
    "    for word_index_1, word_index_2 in zip(corpus_matrix.nonzero()[0], \n",
    "                                          corpus_matrix.nonzero()[1]):\n",
    "\n",
    "        sg_count = corpus_matrix[word_index_1, word_index_2]\n",
    "\n",
    "        pwc = sg_count\n",
    "        pw = sum_over_contexts[word_index_1]\n",
    "        pc = sum_over_words[word_index_2]\n",
    "        \n",
    "        xwc = X[word_index_1, word_index_2]\n",
    "\n",
    "        grad_F[word_index_1, word_index_2] = pwc * sigmoid_fun(-xwc) - k * pw * pc / all_observations * sigmoid_fun(xwc)\n",
    "    \n",
    "    return grad_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riemannian Optimization for SGNS\n",
    "def riemannian_optimization(X=None, corpus_matrix=None, step=0.001, k=3, max_iter=50, alpha=0.5):\n",
    "    \n",
    "    U, S, V = np.linalg.svd(X)\n",
    "    \n",
    "    for i in range(1, max_iter):\n",
    "        \n",
    "        Y = X + step * computer_grad_matrix(X, corpus_matrix, k=k)\n",
    "        _U, _S = np.linalg.qr(Y @ V)\n",
    "        _V, _S = np.linalg.qr(Y.T @ U)\n",
    "        \n",
    "        print('Step', i, 'norm change at step', np.linalg.norm(_U @ _S @ _V - X, ord='fro'))\n",
    "        \n",
    "        X = _U @ _S @ _V\n",
    "        U = _U\n",
    "        V = _V\n",
    "        \n",
    "        \n",
    "    U, S, V = np.linalg.svd(X)\n",
    "    \n",
    "    return U @ np.diag(np.power(S, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 norm change at step 160.4826250411553\n",
      "Step 2 norm change at step 160.27247963108454\n",
      "Step 3 norm change at step 165.76569669509223\n",
      "Step 4 norm change at step 161.72842061628853\n",
      "Step 5 norm change at step 153.74452825779244\n",
      "Step 6 norm change at step 146.5226279797441\n",
      "Step 7 norm change at step 143.2818997825804\n",
      "Step 8 norm change at step 158.05942227407567\n",
      "Step 9 norm change at step 150.16802731917258\n"
     ]
    }
   ],
   "source": [
    "X = np.ones(corpus_matrix[:100, :100].shape)\n",
    "E = riemannian_optimization(X=X, corpus_matrix=corpus_matrix[:100, :100], step=1e-4, max_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for converting Sentences with Word2Vec class into vectors for representing sentences in vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison models of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 base models, which included into our experiments for classifient feedbacks grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
